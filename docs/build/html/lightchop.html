

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Float Precision Simulator Classes &mdash; pychop 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=53e15035" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=d45e8c67"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            pychop
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="start.html">Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="chop.html">Floating point simuluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="mathfunc.html">Mathematical Functions in the Chop Class</a></li>
<li class="toctree-l1"><a class="reference internal" href="quant.html">Integer quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="fix_point.html">Fixed point quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="fix_point.html#fixed-point-simulator-classes">Fixed-Point Simulator Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="nn.html">Quantization aware training</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pychop</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Float Precision Simulator Classes</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/lightchop.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="float-precision-simulator-classes">
<span id="float-precision-simulator"></span><h1>Float Precision Simulator Classes<a class="headerlink" href="#float-precision-simulator-classes" title="Link to this heading"></a></h1>
<p>The <cite>LightChop</cite> class enables quantization of floating-point numbers into a custom floating-point format similar to IEEE 754, defined by a specified number of exponent and mantissa bits. This document outlines the usage and examples of this class across three frameworks: PyTorch, NumPy, and JAX. Each implementation supports six rounding modes:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">-</span> <span class="mi">0</span> <span class="ow">or</span> <span class="s2">&quot;nearest_odd&quot;</span><span class="p">:</span> <span class="n">Round</span> <span class="n">to</span> <span class="n">nearest</span> <span class="n">value</span><span class="p">,</span> <span class="n">ties</span> <span class="n">to</span> <span class="n">odd</span><span class="o">.</span>
<span class="o">-</span> <span class="mi">1</span> <span class="ow">or</span> <span class="s2">&quot;nearest&quot;</span><span class="p">:</span> <span class="n">Round</span> <span class="n">to</span> <span class="n">nearest</span> <span class="n">value</span><span class="p">,</span> <span class="n">ties</span> <span class="n">to</span> <span class="n">even</span> <span class="p">(</span><span class="n">IEEE</span> <span class="mi">754</span> <span class="n">default</span><span class="p">)</span><span class="o">.</span>
<span class="o">-</span> <span class="mi">2</span> <span class="ow">or</span> <span class="s2">&quot;plus_inf&quot;</span><span class="p">:</span> <span class="n">Round</span> <span class="n">towards</span> <span class="n">plus</span> <span class="n">infinity</span> <span class="p">(</span><span class="nb">round</span> <span class="n">up</span><span class="p">)</span><span class="o">.</span>
<span class="o">-</span> <span class="mi">3</span> <span class="ow">or</span> <span class="s2">&quot;minus_inf&quot;</span><span class="p">:</span> <span class="n">Round</span> <span class="n">towards</span> <span class="n">minus</span> <span class="n">infinity</span> <span class="p">(</span><span class="nb">round</span> <span class="n">down</span><span class="p">)</span><span class="o">.</span>
<span class="o">-</span> <span class="mi">4</span> <span class="ow">or</span> <span class="s2">&quot;toward_zero&quot;</span><span class="p">:</span> <span class="n">Truncate</span> <span class="n">toward</span> <span class="n">zero</span> <span class="p">(</span><span class="n">no</span> <span class="n">rounding</span> <span class="n">up</span><span class="p">)</span><span class="o">.</span>
<span class="o">-</span> <span class="mi">5</span> <span class="ow">or</span> <span class="s2">&quot;stoc_prop&quot;</span><span class="p">:</span> <span class="n">Stochastic</span> <span class="n">rounding</span> <span class="n">proportional</span> <span class="n">to</span> <span class="n">the</span> <span class="n">fractional</span> <span class="n">part</span><span class="o">.</span>
<span class="o">-</span> <span class="mi">6</span> <span class="ow">or</span> <span class="s2">&quot;stoc_equal&quot;</span><span class="p">:</span> <span class="n">Stochastic</span> <span class="n">rounding</span> <span class="k">with</span> <span class="mi">50</span><span class="o">%</span> <span class="n">probability</span><span class="o">.</span>
</pre></div>
</div>
<p>This guide offers a practical introduction to the <cite>LightChop</cite> classes, with code examples formatted for clarity and illustrative outputs reflecting the FP16-like behavior</p>
<p>Overview
——–LightChop</p>
<p>The <cite>LightChop</cite> converts floating-point values into a custom floating-point representation with:
- <strong>Exponent Bits</strong>: Determines the range of representable values.
- <strong>Mantissa Bits</strong>: Determines the precision of the fractional part.
- <strong>Format</strong>: For FP16-like settings (5 exponent bits, 10 mantissa bits), the range is approximately ([-65504, 65504]) with a precision of about (2^{-10} = 0.0009765625) for normal numbers.</p>
<p>The quantization process decomposes the input into sign, exponent, and mantissa components, applies the selected rounding mode to the mantissa, and reconstructs the value, handling special cases like zeros, infinities, and NaNs.</p>
<section id="usage">
<h2>Usage<a class="headerlink" href="#usage" title="Link to this heading"></a></h2>
<section id="common-parameters">
<h3>Common Parameters<a class="headerlink" href="#common-parameters" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>exp_bits</strong>: Number of bits for the exponent, defining the dynamic range.</p></li>
<li><p><strong>sig_bits</strong>: Number of bits for the mantissa, defining the precision.</p></li>
<li><p><strong>rmode</strong>: String specifying the rounding method, defaulting to “nearest”.</p></li>
</ul>
</section>
<section id="pytorch-version">
<h3>PyTorch Version<a class="headerlink" href="#pytorch-version" title="Link to this heading"></a></h3>
<p>The PyTorch implementation operates on PyTorch tensors, aligning with IEEE 754 conventions and integrating seamlessly into PyTorch workflows.</p>
<p><strong>Initialization</strong></p>
<p>Create an instance by specifying the number of exponent and mantissa bits, such as 5 and 10 for an FP16-like format.</p>
<p><strong>Quantization</strong></p>
<p>Quantize a tensor by calling the quantization method with the input tensor and an optional rounding mode. The result is a tensor quantized to the custom floating-point format.</p>
<p><strong>Code Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize with 5 exponent bits and 10 mantissa bits (FP16-like)</span>
<span class="n">sim</span> <span class="o">=</span> <span class="n">LightChop</span><span class="p">(</span><span class="n">exp_bits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sig_bits</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="c1"># Input tensor</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="mf">1.7641</span><span class="p">,</span> <span class="mf">0.3097</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2021</span><span class="p">,</span> <span class="mf">2.4700</span><span class="p">,</span> <span class="mf">0.3300</span><span class="p">])</span>
<span class="c1"># Quantize with nearest rounding</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">sim</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">values</span><span class="p">,</span> <span class="n">rmode</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="numpy-version">
<h3>NumPy Version<a class="headerlink" href="#numpy-version" title="Link to this heading"></a></h3>
<p>The NumPy version works with NumPy arrays, providing a general-purpose floating-point quantization tool.</p>
<p><strong>Initialization</strong></p>
<p>Instantiate the simulator with the desired exponent and mantissa bit counts.</p>
<p><strong>Quantization</strong></p>
<p>Apply the quantization method to a NumPy array, optionally specifying a rounding mode, to produce a quantized array.</p>
<p><strong>Code Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize with 5 exponent bits and 10 mantissa bits, half precision</span>
<span class="n">ch</span> <span class="o">=</span> <span class="n">LightChop</span><span class="p">(</span><span class="n">exp_bits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sig_bits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rmode</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
<span class="c1"># Input array</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.7641</span><span class="p">,</span> <span class="mf">0.3097</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2021</span><span class="p">,</span> <span class="mf">2.4700</span><span class="p">,</span> <span class="mf">0.3300</span><span class="p">])</span>
<span class="c1"># Quantize with nearest rounding</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ch</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="jax-version">
<h3>JAX Version<a class="headerlink" href="#jax-version" title="Link to this heading"></a></h3>
<p>The JAX version utilizes JAX arrays and includes JIT compilation for performance, requiring a PRNG key for stochastic rounding modes.</p>
<p><strong>Initialization</strong></p>
<p>Set up the simulator by defining the exponent and mantissa bits.</p>
<p><strong>Quantization</strong></p>
<p>Quantize a JAX array using the quantization method, providing the array, an optional rounding mode, and a PRNG key for stochastic modes. The output is a quantized JAX array.</p>
<p><strong>Code Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Initialize with 5 exponent bits and 10 mantissa bits, half precision</span>
<span class="n">ch</span> <span class="o">=</span> <span class="n">LightChop</span><span class="p">(</span><span class="n">exp_bits</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">sig_bits</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">rmode</span><span class="o">=</span><span class="s2">&quot;nearest&quot;</span><span class="p">)</span>
<span class="c1"># Input array</span>
<span class="n">values</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.7641</span><span class="p">,</span> <span class="mf">0.3097</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2021</span><span class="p">,</span> <span class="mf">2.4700</span><span class="p">,</span> <span class="mf">0.3300</span><span class="p">])</span>
<span class="c1"># PRNG key for stochastic modes</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="c1"># Quantize with nearest rounding (no key needed)</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">ch</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading"></a></h2>
<p>The examples below demonstrate the quantization of the input values <cite>[1.7641, 0.3097, -0.2021, 2.47, 0.33]</cite> using a custom FP16-like format (5 exponent bits, 10 mantissa bits) across all rounding modes. Outputs are based on the PyTorch implementation and should be consistent across frameworks, with stochastic modes varying unless seeded (JAX uses PRNG key 42).</p>
<p><strong>Input Values</strong></p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[1.7641, 0.3097, -0.2021, 2.47, 0.33]
</pre></div>
</div>
<p><strong>Outputs by Rounding Mode</strong></p>
<ul>
<li><p><strong>Nearest</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[1.7637, 0.3098, -0.2020, 2.4707, 0.3301]
</pre></div>
</div>
<p>Rounds the mantissa to the nearest representable value.</p>
</li>
<li><p><strong>Up</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[1.7646, 0.3101, -0.2019, 2.4727, 0.3303]
</pre></div>
</div>
<p>Positive values round toward positive infinity, negative values toward negative infinity.</p>
</li>
<li><p><strong>Down</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[1.7637, 0.3096, -0.2021, 2.4707, 0.3298]
</pre></div>
</div>
<p>Positive values round toward negative infinity, negative values toward positive infinity.</p>
</li>
<li><p><strong>Towards Zero</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[1.7637, 0.3096, -0.2019, 2.4707, 0.3298]
</pre></div>
</div>
<p>Truncates the mantissa toward zero, reducing magnitude.</p>
</li>
<li><p><strong>Stochastic Equal</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[1.7637, 0.3098, -0.2020, 2.4707, 0.3301]  # Example with JAX PRNG key 42
</pre></div>
</div>
<p>Randomly chooses between floor and ceiling with equal probability (varies across runs).</p>
</li>
<li><p><strong>Stochastic Proportional</strong>:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>[1.7646, 0.3098, -0.2019, 2.4707, 0.3301]  # Example with JAX PRNG key 42
</pre></div>
</div>
<p>Randomly chooses between floor and ceiling, with probability proportional to the fractional part (varies across runs).</p>
</li>
</ul>
</section>
<section id="notes">
<h2>Notes<a class="headerlink" href="#notes" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><strong>Comparison to Native FP16</strong>: The “nearest” mode closely matches PyTorch’s native FP16 quantization (e.g., <cite>[1.7637, 0.3098, -0.2020, 2.4707, 0.3301]</cite>), validating the implementation.</p></li>
<li><p><strong>Stochastic Modes</strong>: Outputs for <cite>stochastic_equal</cite> and <cite>stochastic_proportional</cite> depend on random number generation, with JAX requiring a PRNG key for reproducibility, unlike PyTorch and NumPy’s internal randomness.</p></li>
<li><p><strong>Special Cases</strong>: All versions handle zeros, infinities, and NaNs appropriately, preserving IEEE 754-like behavior.</p></li>
</ul>
<p>.</p>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, InEXASCALE computing.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>