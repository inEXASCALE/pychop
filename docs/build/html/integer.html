

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Integer quantization &mdash; pychop 0.0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=53e15035" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/jquery.js?v=5d32c60e"></script>
      <script src="_static/_sphinx_javascript_frameworks_compat.js?v=2cd50e6c"></script>
      <script src="_static/documentation_options.js?v=d45e8c67"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Fixed point quantization" href="fix_point.html" />
    <link rel="prev" title="Mathematical Functions" href="mathfunc.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            pychop
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="start.html">Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="chop.html">Floating point simuluation</a></li>
<li class="toctree-l1"><a class="reference internal" href="lightchop.html">Float Precision Simulator Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="mathfunc.html">Mathematical Functions</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Integer quantization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#basic-usage">Basic usage</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#Chopi"><code class="docutils literal notranslate"><span class="pre">Chopi</span></code></a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Chopi.calibrate"><code class="docutils literal notranslate"><span class="pre">Chopi.calibrate()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#Chopi.quantize"><code class="docutils literal notranslate"><span class="pre">Chopi.quantize()</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#Chopi.dequantize"><code class="docutils literal notranslate"><span class="pre">Chopi.dequantize()</span></code></a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#quantization-aware-training">Quantization aware training</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="fix_point.html">Fixed point quantization</a></li>
<li class="toctree-l1"><a class="reference internal" href="fix_point.html#fixed-point-simulator-classes">Fixed-Point Simulator Classes</a></li>
<li class="toctree-l1"><a class="reference internal" href="layers.html">Quantized Layers Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="optimizers.html">Quantized Optimizers Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pychop</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Integer quantization</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/integer.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="integer-quantization">
<h1>Integer quantization<a class="headerlink" href="#integer-quantization" title="Link to this heading"></a></h1>
<p>Integer quantization is another important feature of <code class="docutils literal notranslate"><span class="pre">pychop</span></code>. It intention is to convert the floating point number into
low bit-width integer, which speedup the computations in certain computing hardware. It performs quantization with
user-defined bitwidths. The following example illustrates the usage of the method.</p>
<section id="basic-usage">
<h2>Basic usage<a class="headerlink" href="#basic-usage" title="Link to this heading"></a></h2>
<p>The integer arithmetic emulation of <code class="docutils literal notranslate"><span class="pre">pychop</span></code> is implemented by the interface <code class="docutils literal notranslate"><span class="pre">Chopi</span></code>. It can be used in many circumstances, and offer flexible choices for users to choose, such as symmetric quantization or not, number of bitwidth to use, the usage is illustrated as below:</p>
<dl class="py class">
<dt class="sig sig-object py" id="Chopi">
<em class="property"><span class="pre">class</span><span class="w"> </span></em><span class="sig-name descname"><span class="pre">Chopi</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">num_bits</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">symmetric</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_channel</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">channel_dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Chopi" title="Link to this definition"></a></dt>
<dd><p>A class for quantizing and dequantizing arrays to and from integer representations.</p>
<p>This class supports both symmetric and asymmetric quantization, with optional per-channel quantization along a specified axis. It is designed for inference-style quantization in JAX, PyTorch, and NumPy frameworks, with framework-specific array types (<code class="docutils literal notranslate"><span class="pre">jnp.ndarray</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code>, <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code>).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>num_bits</strong> (<em>int</em>) – Bit-width for quantization (e.g., 8 for INT8). Default is 8.</p></li>
<li><p><strong>symmetric</strong> (<em>bool</em>) – If True, use symmetric quantization (zero_point = 0). If False, use asymmetric quantization. Default is False.</p></li>
<li><p><strong>per_channel</strong> (<em>bool</em>) – If True, quantize per channel along the specified <code class="docutils literal notranslate"><span class="pre">channel_dim</span></code>. If False, quantize the entire array. Default is False.</p></li>
<li><p><strong>channel_dim</strong> (<em>int</em>) – Dimension to treat as the channel axis for per-channel quantization. Default is 0.</p></li>
</ul>
</dd>
<dt class="field-even">Variables<span class="colon">:</span></dt>
<dd class="field-even"><ul class="simple">
<li><p><strong>qmin</strong> (<em>int</em>) – Minimum quantized value (e.g., -128 for symmetric INT8, 0 for asymmetric INT8).</p></li>
<li><p><strong>qmax</strong> (<em>int</em>) – Maximum quantized value (e.g., 127 for INT8).</p></li>
<li><p><strong>scale</strong> – Scaling factor(s) for quantization, computed during calibration. Shape depends on <code class="docutils literal notranslate"><span class="pre">per_channel</span></code> (scalar or array).</p></li>
<li><p><strong>zero_point</strong> – Zero-point offset(s) for quantization, computed during calibration. None if symmetric, else matches <code class="docutils literal notranslate"><span class="pre">scale</span></code> shape.</p></li>
</ul>
</dd>
</dl>
<dl class="py method">
<dt class="sig sig-object py" id="Chopi.calibrate">
<span class="sig-name descname"><span class="pre">calibrate</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Chopi.calibrate" title="Link to this definition"></a></dt>
<dd><p>Calibrate the Chopi by computing the scale and zero-point based on the input array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Input array to calibrate from (<code class="docutils literal notranslate"><span class="pre">jnp.ndarray</span></code> for JAX, <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> for PyTorch, <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> for NumPy).</p>
</dd>
<dt class="field-even">Raises<span class="colon">:</span></dt>
<dd class="field-even"><p><strong>TypeError</strong> – If the input is not of the expected array type for the framework.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Chopi.quantize">
<span class="sig-name descname"><span class="pre">quantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">x</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Chopi.quantize" title="Link to this definition"></a></dt>
<dd><p>Quantize the input array to integers.</p>
<p>If the Chopi has not been calibrated, it will automatically calibrate using the input array.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>x</strong> – Input array to quantize (<code class="docutils literal notranslate"><span class="pre">jnp.ndarray</span></code> for JAX, <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> for PyTorch, <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> for NumPy).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Quantized integer array (<code class="docutils literal notranslate"><span class="pre">jnp.ndarray</span></code> with dtype <code class="docutils literal notranslate"><span class="pre">int8</span></code> for JAX, <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> with dtype <code class="docutils literal notranslate"><span class="pre">torch.int8</span></code> for PyTorch, <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> with dtype <code class="docutils literal notranslate"><span class="pre">int8</span></code> for NumPy).</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>TypeError</strong> – If the input is not of the expected array type for the framework.</p>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="Chopi.dequantize">
<span class="sig-name descname"><span class="pre">dequantize</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">q</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#Chopi.dequantize" title="Link to this definition"></a></dt>
<dd><p>Dequantize the integer array back to floating-point.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><p><strong>q</strong> – Quantized integer array (<code class="docutils literal notranslate"><span class="pre">jnp.ndarray</span></code> for JAX, <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> for PyTorch, <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> for NumPy).</p>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><p>Dequantized floating-point array (<code class="docutils literal notranslate"><span class="pre">jnp.ndarray</span></code> for JAX, <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> for PyTorch, <code class="docutils literal notranslate"><span class="pre">np.ndarray</span></code> for NumPy).</p>
</dd>
<dt class="field-odd">Raises<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>TypeError</strong> – If the input is not of the expected array type for the framework.</p></li>
<li><p><strong>ValueError</strong> – If the Chopi has not been calibrated (i.e., <code class="docutils literal notranslate"><span class="pre">scale</span></code> is None).</p></li>
</ul>
</dd>
</dl>
</dd></dl>

<p class="rubric">Principle</p>
<p>Quantization reduces the precision of floating-point values to integers to save memory and accelerate computation, especially on hardware with integer arithmetic support. The process involves:</p>
<ol class="arabic simple">
<li><p><strong>Calibration</strong>: Determine the range of the input array (min and max values) to compute a scaling factor (<code class="docutils literal notranslate"><span class="pre">scale</span></code>) and offset (<code class="docutils literal notranslate"><span class="pre">zero_point</span></code>).</p></li>
<li><p><strong>Quantization</strong>: Map floats to integers using <code class="docutils literal notranslate"><span class="pre">q</span> <span class="pre">=</span> <span class="pre">round(x</span> <span class="pre">/</span> <span class="pre">scale</span> <span class="pre">+</span> <span class="pre">zero_point)</span></code>, clipped to <code class="docutils literal notranslate"><span class="pre">[qmin,</span> <span class="pre">qmax]</span></code>.</p></li>
<li><p><strong>Dequantization</strong>: Recover approximate floats using <code class="docutils literal notranslate"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">(q</span> <span class="pre">-</span> <span class="pre">zero_point)</span> <span class="pre">*</span> <span class="pre">scale</span></code>.</p></li>
</ol>
<ul class="simple">
<li><p><strong>Symmetric</strong>: Assumes <code class="docutils literal notranslate"><span class="pre">zero_point</span> <span class="pre">=</span> <span class="pre">0</span></code> (e.g., range <code class="docutils literal notranslate"><span class="pre">[-128,</span> <span class="pre">127]</span></code> for INT8), suitable for weights with zero-centered distributions.</p></li>
<li><p><strong>Asymmetric</strong>: Allows <code class="docutils literal notranslate"><span class="pre">zero_point</span></code> to shift the range (e.g., <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">255]</span></code> for INT8), better for activations with non-zero minima.</p></li>
<li><p><strong>Per-channel</strong>: Applies separate <code class="docutils literal notranslate"><span class="pre">scale</span></code> and <code class="docutils literal notranslate"><span class="pre">zero_point</span></code> per channel, improving accuracy for multi-channel data (e.g., CNN weights).</p></li>
</ul>
<p class="rubric">Examples</p>
<p><strong>JAX Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">pychop</span> <span class="kn">import</span> <span class="n">Chopi</span>  <span class="c1"># Assuming module name</span>
<span class="n">pychop</span><span class="o">.</span><span class="n">backend</span><span class="p">(</span><span class="s1">&#39;jax&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]])</span>
<span class="n">Chopi</span> <span class="o">=</span> <span class="n">Chopi</span><span class="p">(</span><span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">Chopi</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">dq</span> <span class="o">=</span> <span class="n">Chopi</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># e.g., [[ 85  42] [106 127]], dtype=int8</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dq</span><span class="p">)</span>  <span class="c1"># e.g., [[ 0.098  -0.196] [ 0.294   0.392]]</span>
</pre></div>
</div>
<p><strong>PyTorch Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">pychop</span> <span class="kn">import</span> <span class="n">Chopi</span>  <span class="c1"># Assuming module name</span>
<span class="n">pychop</span><span class="o">.</span><span class="n">backend</span><span class="p">(</span><span class="s1">&#39;torch&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]])</span>
<span class="n">Chopi</span> <span class="o">=</span> <span class="n">Chopi</span><span class="p">(</span><span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">Chopi</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Inference mode</span>
<span class="n">dq</span> <span class="o">=</span> <span class="n">Chopi</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># e.g., tensor([[ 85,  42], [106, 127]], dtype=torch.int8)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dq</span><span class="p">)</span>  <span class="c1"># e.g., tensor([[ 0.098, -0.196], [ 0.294,  0.392]])</span>
</pre></div>
</div>
<p><strong>NumPy Example</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">pychop</span> <span class="kn">import</span> <span class="n">Chopi</span>  <span class="c1"># Assuming module name</span>
<span class="n">pychop</span><span class="o">.</span><span class="n">backend</span><span class="p">(</span><span class="s1">&#39;numpy&#39;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]])</span>
<span class="n">Chopi</span> <span class="o">=</span> <span class="n">NumpyChopi</span><span class="p">(</span><span class="n">num_bits</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">symmetric</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">Chopi</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">dq</span> <span class="o">=</span> <span class="n">Chopi</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">q</span><span class="p">)</span>  <span class="c1"># e.g., [[ 85  42] [106 127]], dtype=int8</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dq</span><span class="p">)</span>  <span class="c1"># e.g., [[ 0.098  -0.196] [ 0.294   0.392]]</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<ul class="simple">
<li><p>The PyTorch version supports training mode via <code class="docutils literal notranslate"><span class="pre">forward(x,</span> <span class="pre">training=True)</span></code> for fake quantization, which isn’t shown here but is useful for quantization-aware training.</p></li>
<li><p>Exact integer values may vary slightly due to rounding and range differences.</p></li>
</ul>
</div>
</dd></dl>

<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">pychop</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">linalg</span>
<span class="kn">import</span> <span class="nn">jax</span>

<span class="n">X_np</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="mi">500</span><span class="p">)</span> <span class="c1"># NumPy array</span>
<span class="n">X_th</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">X_np</span><span class="p">)</span> <span class="c1"># Torch array</span>
<span class="n">X_jx</span> <span class="o">=</span> <span class="n">jax</span><span class="o">.</span><span class="n">numpy</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">X_np</span><span class="p">)</span> <span class="c1"># JAX array</span>
<span class="nb">print</span><span class="p">(</span><span class="n">X_np</span><span class="p">)</span>

<span class="n">pychop</span><span class="o">.</span><span class="n">backend</span><span class="p">(</span><span class="s1">&#39;numpy&#39;</span><span class="p">)</span>
<span class="n">pyq_f</span> <span class="o">=</span> <span class="n">pychop</span><span class="o">.</span><span class="n">Chopi</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span> <span class="c1"># The larger the ``bits`` are, the more accurate of the reconstruction is</span>
<span class="n">X_q</span> <span class="o">=</span> <span class="n">pyq_f</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">X_np</span><span class="p">)</span> <span class="c1"># quant array -&gt; integer</span>
<span class="n">X_inv</span> <span class="o">=</span> <span class="n">pyq_f</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="n">X_q</span><span class="p">)</span> <span class="c1"># dequant array -&gt; floating point values</span>
<span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X_inv</span> <span class="o">-</span> <span class="n">X_np</span><span class="p">)</span>


<span class="n">pychop</span><span class="o">.</span><span class="n">backend</span><span class="p">(</span><span class="s1">&#39;torch&#39;</span><span class="p">)</span>
<span class="n">pyq_f</span> <span class="o">=</span> <span class="n">pychop</span><span class="o">.</span><span class="n">Chopi</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">X_q</span> <span class="o">=</span> <span class="n">pyq_f</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">X_th</span><span class="p">)</span>  <span class="c1"># quant array -&gt; integer</span>
<span class="n">X_inv</span> <span class="o">=</span> <span class="n">pyq_f</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="n">X_q</span><span class="p">)</span> <span class="c1"># dequant array -&gt; floating point values</span>
<span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X_inv</span> <span class="o">-</span> <span class="n">X_np</span><span class="p">)</span>


<span class="n">pychop</span><span class="o">.</span><span class="n">backend</span><span class="p">(</span><span class="s1">&#39;jax&#39;</span><span class="p">)</span>
<span class="n">pyq_f</span> <span class="o">=</span> <span class="n">pychop</span><span class="o">.</span><span class="n">Chopi</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">X_q</span> <span class="o">=</span> <span class="n">pyq_f</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">X_jx</span><span class="p">)</span> <span class="c1"># quant array -&gt; integer</span>
<span class="n">X_inv</span> <span class="o">=</span> <span class="n">pyq_f</span><span class="o">.</span><span class="n">dequantize</span><span class="p">(</span><span class="n">X_q</span><span class="p">)</span> <span class="c1"># dequant array -&gt; floating point values</span>
<span class="n">linalg</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">X_inv</span> <span class="o">-</span> <span class="n">X_jx</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="quantization-aware-training">
<h2>Quantization aware training<a class="headerlink" href="#quantization-aware-training" title="Link to this heading"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">pychop</span></code> provides easy-to-use API for quantization aware training.</p>
<p>Simply load the module via:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pychop</span> <span class="kn">import</span> <span class="n">QuantLayer</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">QuantLayer</span></code> enables the quantization components of <code class="docutils literal notranslate"><span class="pre">quant</span></code>, <code class="docutils literal notranslate"><span class="pre">chop</span></code>, and <code class="docutils literal notranslate"><span class="pre">fixed_point</span></code> to be integrated into neural network training,
which is often referred to as quantization-aware training.</p>
<div class="admonition-note admonition">
<p class="admonition-title">Note</p>
<p>The QuantLayer only support backend of Torch, so as to successfully run this functionality, please use</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pychop</span><span class="o">.</span><span class="n">backend</span><span class="p">(</span><span class="s1">&#39;torch&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The usage of QuantLayer simply extended by the <code class="docutils literal notranslate"><span class="pre">quant</span></code>, <code class="docutils literal notranslate"><span class="pre">chop</span></code>, and <code class="docutils literal notranslate"><span class="pre">fixed_point</span></code>, therefore, we need to first load the corresponding modules via:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">pychop</span> <span class="kn">import</span> <span class="n">quant</span>
<span class="kn">from</span> <span class="nn">pychop</span> <span class="kn">import</span> <span class="n">chop</span>
<span class="kn">from</span> <span class="nn">pychop</span> <span class="kn">import</span> <span class="n">fpoint</span>
</pre></div>
</div>
<p>The quantization-aware training simply perform by plugging the <code class="docutils literal notranslate"><span class="pre">QuantLayer</span></code> into neural network building. We illustrate its usage in fully connected layer training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">NeuralNet</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">NeuralNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant1</span> <span class="o">=</span> <span class="n">QuantLayer</span><span class="p">(</span><span class="n">fpoint</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="mi">4</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant2</span> <span class="o">=</span> <span class="n">QuantLayer</span><span class="p">(</span><span class="n">chop</span><span class="p">(</span><span class="s1">&#39;h&#39;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quant3</span> <span class="o">=</span> <span class="n">QuantLayer</span><span class="p">(</span><span class="n">quant</span><span class="p">())</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant1</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">quant3</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc2</span><span class="p">(</span><span class="n">out</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">out</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="mathfunc.html" class="btn btn-neutral float-left" title="Mathematical Functions" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="fix_point.html" class="btn btn-neutral float-right" title="Fixed point quantization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2024, InEXASCALE computing.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>