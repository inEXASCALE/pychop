{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296dd14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import pychop as pychop\n",
    "from pychop.chop import chop\n",
    "from pychop import float_params\n",
    "from time import time\n",
    "from numpy import linalg\n",
    "import jax\n",
    "# from pychop.chop import chop\n",
    "# from pychop.quant import quant\n",
    "from time import time\n",
    "from scipy.io import savemat\n",
    "# np.set_printoptions(precision=19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ad8e9d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.76405235  0.40015721  0.97873798 ...  0.92918181  0.22941801\n",
      "   0.41440588]\n",
      " [ 0.30972382 -0.73745619 -1.53691988 ...  0.51687218 -0.03292069\n",
      "   1.29811143]\n",
      " [-0.20211703 -0.833231    1.73360025 ...  0.75309415 -0.58103281\n",
      "  -0.19837974]\n",
      " ...\n",
      " [ 1.07432182  1.188486    0.5092741  ...  0.07053449  0.59975911\n",
      "  -2.41029925]\n",
      " [ 0.32432475 -0.02337844  1.62873399 ... -0.16088168 -1.59772992\n",
      "   1.414703  ]\n",
      " [ 0.63460807  1.38090977  0.54829109 ...  0.30762729 -0.11078251\n",
      "   0.83859307]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "X_np = np.random.randn(5000, 5000) # Numpy array\n",
    "X_th = torch.Tensor(X_np) # torch array\n",
    "X_jx = jax.numpy.asarray(X_np)\n",
    "print(X_np)\n",
    "\n",
    "#savemat(\"tests/verify.mat\", {\"X\":X_np})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8962a603-dbb8-4992-924c-4fd7dcbd3ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7641,  0.3097, -0.2021,  2.4700,  0.3300])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_th[0:5, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b51fba",
   "metadata": {},
   "source": [
    "### print unit-roundoff in machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "618838ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>u</th>\n",
       "      <th>xmins</th>\n",
       "      <th>xmin</th>\n",
       "      <th>xmax</th>\n",
       "      <th>p</th>\n",
       "      <th>emins</th>\n",
       "      <th>emin</th>\n",
       "      <th>emax</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q43</td>\n",
       "      <td>6.25e-02</td>\n",
       "      <td>1.95e-03</td>\n",
       "      <td>1.56e-02</td>\n",
       "      <td>2.40e+02</td>\n",
       "      <td>4</td>\n",
       "      <td>-9</td>\n",
       "      <td>-6</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q52</td>\n",
       "      <td>1.25e-01</td>\n",
       "      <td>1.53e-05</td>\n",
       "      <td>6.10e-05</td>\n",
       "      <td>5.73e+04</td>\n",
       "      <td>3</td>\n",
       "      <td>-16</td>\n",
       "      <td>-14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b</td>\n",
       "      <td>3.91e-03</td>\n",
       "      <td>9.18e-41</td>\n",
       "      <td>1.18e-38</td>\n",
       "      <td>3.39e+38</td>\n",
       "      <td>8</td>\n",
       "      <td>-133</td>\n",
       "      <td>-126</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>h</td>\n",
       "      <td>4.88e-04</td>\n",
       "      <td>5.96e-08</td>\n",
       "      <td>6.10e-05</td>\n",
       "      <td>6.55e+04</td>\n",
       "      <td>11</td>\n",
       "      <td>-24</td>\n",
       "      <td>-14</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t</td>\n",
       "      <td>4.88e-04</td>\n",
       "      <td>1.15e-41</td>\n",
       "      <td>1.18e-38</td>\n",
       "      <td>3.40e+38</td>\n",
       "      <td>11</td>\n",
       "      <td>-136</td>\n",
       "      <td>-126</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>s</td>\n",
       "      <td>5.96e-08</td>\n",
       "      <td>1.40e-45</td>\n",
       "      <td>1.18e-38</td>\n",
       "      <td>3.40e+38</td>\n",
       "      <td>24</td>\n",
       "      <td>-149</td>\n",
       "      <td>-126</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>d</td>\n",
       "      <td>1.11e-16</td>\n",
       "      <td>4.94e-324</td>\n",
       "      <td>2.23e-308</td>\n",
       "      <td>1.80e+308</td>\n",
       "      <td>53</td>\n",
       "      <td>-1074</td>\n",
       "      <td>-1022</td>\n",
       "      <td>1023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>q</td>\n",
       "      <td>9.63e-35</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>0.00e+00</td>\n",
       "      <td>inf</td>\n",
       "      <td>113</td>\n",
       "      <td>-16494</td>\n",
       "      <td>-16382</td>\n",
       "      <td>16383</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                u      xmins       xmin       xmax    p    emins     emin  \\\n",
       "0  q43   6.25e-02   1.95e-03   1.56e-02   2.40e+02    4       -9       -6   \n",
       "1  q52   1.25e-01   1.53e-05   6.10e-05   5.73e+04    3      -16      -14   \n",
       "2    b   3.91e-03   9.18e-41   1.18e-38   3.39e+38    8     -133     -126   \n",
       "3    h   4.88e-04   5.96e-08   6.10e-05   6.55e+04   11      -24      -14   \n",
       "4    t   4.88e-04   1.15e-41   1.18e-38   3.40e+38   11     -136     -126   \n",
       "5    s   5.96e-08   1.40e-45   1.18e-38   3.40e+38   24     -149     -126   \n",
       "6    d   1.11e-16  4.94e-324  2.23e-308  1.80e+308   53    -1074    -1022   \n",
       "7    q   9.63e-35   0.00e+00   0.00e+00        inf  113   -16494   -16382   \n",
       "\n",
       "     emax  \n",
       "0       7  \n",
       "1      15  \n",
       "2     127  \n",
       "3      15  \n",
       "4     127  \n",
       "5     127  \n",
       "6    1023  \n",
       "7   16383  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e72fb9",
   "metadata": {},
   "source": [
    "### set backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9297600d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load NumPy backend.\n"
     ]
    }
   ],
   "source": [
    "# pychop.backend('torch')\n",
    "pychop.backend('numpy', 1) # print information, NumPy is the default option."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417a000",
   "metadata": {},
   "source": [
    "### run chop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ade7038-b100-46ee-8f12-45147d0f2047",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runtime: 2.001610040664673\n",
      "tensor([ 1.7637,  0.3098, -0.2021,  2.4707,  0.3301,  0.3020,  0.3713,  0.3872,\n",
      "        -1.9395,  0.5615])\n"
     ]
    }
   ],
   "source": [
    "pyq_f = chop('h')\n",
    "st = time()\n",
    "X_bit = pyq_f(X_np)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56faf742-e8cf-4665-bb8a-eaf6f93f43de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Troch backend.\n",
      "runtime: 1.8302061557769775\n",
      "tensor([ 1.7637,  0.3098, -0.2021,  2.4707,  0.3301,  0.3020,  0.3713,  0.3872,\n",
      "        -1.9395,  0.5615])\n"
     ]
    }
   ],
   "source": [
    "pychop.backend('torch', 1) # print information\n",
    "pyq_f = chop('h', rmode=1)\n",
    "st = time()\n",
    "X_bit = pyq_f(X_th)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7def80e-0e97-415d-aad8-cb7c510a422b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Troch backend.\n",
      "runtime: 1.3260867595672607\n",
      "tensor([ 1.7646,  0.3098, -0.2020,  2.4707,  0.3301,  0.3022,  0.3713,  0.3875,\n",
      "        -1.9395,  0.5620])\n"
     ]
    }
   ],
   "source": [
    "pychop.backend('torch', 1) # print information\n",
    "pyq_f = chop('h', rmode=2)\n",
    "st = time()\n",
    "X_bit = pyq_f(X_th)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "504c13ad-7b33-44a0-aa02-b043ad4ee304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Troch backend.\n",
      "runtime: 0.8530240058898926\n",
      "tensor([ 1.7637,  0.3096, -0.2021,  2.4688,  0.3298,  0.3020,  0.3711,  0.3872,\n",
      "        -1.9404,  0.5615])\n"
     ]
    }
   ],
   "source": [
    "pychop.backend('torch', 1) # print information\n",
    "pyq_f = chop('h', rmode=3)\n",
    "st = time()\n",
    "X_bit = pyq_f(X_th)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e283b85-043e-46d6-b0f1-f442255681fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Troch backend.\n",
      "runtime: 1.5211341381072998\n",
      "tensor([ 1.7637,  0.3096, -0.2020,  2.4688,  0.3298,  0.3020,  0.3711,  0.3872,\n",
      "        -1.9395,  0.5615])\n"
     ]
    }
   ],
   "source": [
    "pychop.backend('torch', 1) # print information\n",
    "pyq_f = chop('h', rmode=4)\n",
    "st = time()\n",
    "X_bit = pyq_f(X_th)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9261a2ee-38e8-44b5-8bd2-ad74f9782d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7641,  0.4002,  0.9787,  ...,  0.9292,  0.2294,  0.4144],\n",
       "        [ 0.3097, -0.7375, -1.5369,  ...,  0.5169, -0.0329,  1.2981],\n",
       "        [-0.2021, -0.8332,  1.7336,  ...,  0.7531, -0.5810, -0.1984],\n",
       "        ...,\n",
       "        [ 1.0743,  1.1885,  0.5093,  ...,  0.0705,  0.5998, -2.4103],\n",
       "        [ 0.3243, -0.0234,  1.6287,  ..., -0.1609, -1.5977,  1.4147],\n",
       "        [ 0.6346,  1.3809,  0.5483,  ...,  0.3076, -0.1108,  0.8386]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nearest             : tensor([ 1.7637,  0.3098, -0.2021,  2.4707,  0.3301])\n",
    "up                  : tensor([ 1.7646,  0.3098, -0.2020,  2.4707,  0.3301])\n",
    "down                : tensor([ 1.7637,  0.3096, -0.2021,  2.4688,  0.3298])\n",
    "towards_zero        : tensor([ 1.7637,  0.3096, -0.2020,  2.4688,  0.3298])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3a9234-2b01-44bc-adf7-2a8daf6adc45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5414b447-0848-4900-a6ff-ec9968aa8fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original values: tensor([ 0.1000,  0.3000,  1.7000,  3.9000, -2.5000])\n",
      "nearest: tensor([ 0.1000,  0.3000,  1.7002,  3.9004, -2.5000])\n",
      "up: tensor([ 0.1000,  0.3000,  1.7002,  3.9004, -2.5000])\n",
      "down: tensor([ 0.1000,  0.2998,  1.6992,  3.8984, -2.5000])\n",
      "towards_zero: tensor([ 0.1000,  0.2998,  1.6992,  3.8984, -2.5000])\n",
      "stochastic_equal: tensor([ 0.1000,  0.3000,  1.7002,  3.8984, -2.5000])\n",
      "stochastic_proportional: tensor([ 0.1000,  0.3000,  1.7002,  3.9004, -2.5000])\n",
      "\n",
      "Layer output shape: torch.Size([3, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "class FloatPrecisionSimulator:\n",
    "    \"\"\"\n",
    "    A class to simulate different floating-point precisions and rounding modes\n",
    "    for PyTorch tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, exponent_bits: int, mantissa_bits: int):\n",
    "        self.exponent_bits = exponent_bits\n",
    "        self.mantissa_bits = mantissa_bits\n",
    "        self.max_exp = 2 ** (exponent_bits - 1) - 1\n",
    "        self.min_exp = -self.max_exp + 1\n",
    "        self.bias = 2 ** (exponent_bits - 1) - 1  # Bias for IEEE 754\n",
    "        \n",
    "    def _to_custom_float(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, \n",
    "                                                        torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Initialize with specific format parameters.\n",
    "        Convert to custom float representation with proper IEEE 754 handling\n",
    "        \n",
    "        Args:\n",
    "            exponent_bits: Number of bits for exponent\n",
    "            mantissa_bits: Number of bits for mantissa (significant digits)\n",
    "        \"\"\"\n",
    "        sign = torch.sign(x)\n",
    "        abs_x = torch.abs(x)\n",
    "        \n",
    "        # Handle special cases\n",
    "        zero_mask = (abs_x == 0)\n",
    "        inf_mask = torch.isinf(x)\n",
    "        nan_mask = torch.isnan(x)\n",
    "        \n",
    "        # Calculate raw exponent and mantissa\n",
    "        exponent = torch.floor(torch.log2(abs_x.clamp(min=2.0**-24)))  # Minimum denormal\n",
    "        \n",
    "        # Normalize mantissa to [1, 2)\n",
    "        mantissa = abs_x / (2.0 ** exponent)\n",
    "        \n",
    "        # Handle subnormals\n",
    "        subnormal_mask = (exponent < self.min_exp)\n",
    "        if torch.any(subnormal_mask):\n",
    "            mantissa[subnormal_mask] = abs_x[subnormal_mask] / (2.0 ** self.min_exp)\n",
    "            exponent[subnormal_mask] = self.min_exp\n",
    "        \n",
    "        return sign, exponent + self.bias, mantissa, zero_mask, inf_mask, nan_mask\n",
    "    \n",
    "    def _quantize_components(self, \n",
    "                           x: torch.Tensor,\n",
    "                           sign: torch.Tensor, \n",
    "                           exponent: torch.Tensor, \n",
    "                           mantissa: torch.Tensor,\n",
    "                           zero_mask: torch.Tensor,\n",
    "                           inf_mask: torch.Tensor,\n",
    "                           nan_mask: torch.Tensor,\n",
    "                           rounding_mode: str) -> torch.Tensor:\n",
    "        \"\"\"Quantize components according to IEEE 754 FP16 rules with various rounding modes\"\"\"\n",
    "        \n",
    "        # Clamp exponent to representable range (including bias)\n",
    "        exp_min = 0  # 0 represents subnormals\n",
    "        exp_max = 2**self.exponent_bits - 1  # 31 for FP16\n",
    "        exponent = exponent.clamp(min=exp_min, max=exp_max)\n",
    "        \n",
    "        # Quantize mantissa\n",
    "        mantissa_steps = 2 ** self.mantissa_bits\n",
    "        normal_mask = (exponent > 0) & (exponent < exp_max)\n",
    "        subnormal_mask = (exponent == 0)\n",
    "        mantissa_normal = mantissa - 1.0  # Remove implicit leading 1 for normal numbers\n",
    "        \n",
    "        # Apply rounding mode\n",
    "        if rounding_mode == \"nearest\":\n",
    "            mantissa_q = torch.round(mantissa_normal * mantissa_steps) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.round(mantissa[subnormal_mask] * \n",
    "                                                       mantissa_steps) / mantissa_steps\n",
    "        elif rounding_mode == \"up\":\n",
    "            mantissa_q = torch.where(sign > 0, \n",
    "                                   torch.ceil(mantissa_normal * mantissa_steps),\n",
    "                                   torch.floor(mantissa_normal * mantissa_steps)) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.where(sign[subnormal_mask] > 0,\n",
    "                                                       torch.ceil(mantissa[subnormal_mask] * mantissa_steps),\n",
    "                                                       torch.floor(mantissa[subnormal_mask] * mantissa_steps)) / mantissa_steps\n",
    "        elif rounding_mode == \"down\":\n",
    "            mantissa_q = torch.where(sign > 0,\n",
    "                                   torch.floor(mantissa_normal * mantissa_steps),\n",
    "                                   torch.ceil(mantissa_normal * mantissa_steps)) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.where(sign[subnormal_mask] > 0,\n",
    "                                                       torch.floor(mantissa[subnormal_mask] * mantissa_steps),\n",
    "                                                       torch.ceil(mantissa[subnormal_mask] * mantissa_steps)) / mantissa_steps\n",
    "        elif rounding_mode == \"towards_zero\":\n",
    "            mantissa_q = torch.trunc(mantissa_normal * mantissa_steps) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.trunc(mantissa[subnormal_mask] * \n",
    "                                                       mantissa_steps) / mantissa_steps\n",
    "        elif rounding_mode == \"stochastic_equal\":\n",
    "            mantissa_scaled = mantissa_normal * mantissa_steps\n",
    "            floor_val = torch.floor(mantissa_scaled)\n",
    "            prob = torch.rand_like(mantissa_scaled)\n",
    "            mantissa_q = torch.where(prob < 0.5, floor_val, floor_val + 1) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_scaled = mantissa[subnormal_mask] * mantissa_steps\n",
    "                floor_val = torch.floor(mantissa_scaled)\n",
    "                prob = torch.rand_like(mantissa_scaled)\n",
    "                mantissa_q[subnormal_mask] = torch.where(prob < 0.5, floor_val, \n",
    "                                                       floor_val + 1) / mantissa_steps\n",
    "        elif rounding_mode == \"stochastic_proportional\":\n",
    "            mantissa_scaled = mantissa_normal * mantissa_steps\n",
    "            floor_val = torch.floor(mantissa_scaled)\n",
    "            fraction = mantissa_scaled - floor_val\n",
    "            prob = torch.rand_like(mantissa_scaled)\n",
    "            mantissa_q = torch.where(prob < fraction, floor_val + 1, floor_val) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_scaled = mantissa[subnormal_mask] * mantissa_steps\n",
    "                floor_val = torch.floor(mantissa_scaled)\n",
    "                fraction = mantissa_scaled - floor_val\n",
    "                prob = torch.rand_like(mantissa_scaled)\n",
    "                mantissa_q[subnormal_mask] = torch.where(prob < fraction, floor_val + 1, \n",
    "                                                       floor_val) / mantissa_steps\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported rounding mode: {rounding_mode}\")\n",
    "        \n",
    "        # Reconstruct the number\n",
    "        result = torch.zeros_like(x)\n",
    "        \n",
    "        # Normal numbers\n",
    "        if torch.any(normal_mask):\n",
    "            result[normal_mask] = sign[normal_mask] * (1.0 + mantissa_q[normal_mask]) * \\\n",
    "                                (2.0 ** (exponent[normal_mask] - self.bias))\n",
    "        \n",
    "        # Subnormal numbers\n",
    "        if torch.any(subnormal_mask):\n",
    "            result[subnormal_mask] = sign[subnormal_mask] * mantissa_q[subnormal_mask] * \\\n",
    "                                   (2.0 ** self.min_exp)\n",
    "        \n",
    "        # Special cases\n",
    "        result[zero_mask] = 0.0\n",
    "        result[inf_mask] = torch.sign(x[inf_mask]) * float('inf')\n",
    "        result[nan_mask] = float('nan')\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def quantize(self, x: torch.Tensor, rounding_mode: str = \"nearest\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Quantize a tensor to the specified precision with given rounding mode.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            rounding_mode: One of 'nearest', 'up', 'down', 'towards_zero', \n",
    "                          'stochastic_equal', 'stochastic_proportional'\n",
    "        \"\"\"\n",
    "        sign, exponent, mantissa, zero_mask, inf_mask, nan_mask = self._to_custom_float(x)\n",
    "        return self._quantize_components(x, sign, exponent, mantissa, zero_mask, inf_mask, nan_mask, rounding_mode)\n",
    "\n",
    "\n",
    "class QuantizedLayer(torch.nn.Module):\n",
    "    \"\"\"Example of a quantized linear layer\"\"\"\n",
    "    def __init__(self, \n",
    "                 in_features: int, \n",
    "                 out_features: int,\n",
    "                 exponent_bits: int,\n",
    "                 mantissa_bits: int,\n",
    "                 rounding_mode: str = \"nearest\"):\n",
    "        super().__init__()\n",
    "        self.quantizer = FloatPrecisionSimulator(exponent_bits, mantissa_bits)\n",
    "        self.weight = torch.nn.Parameter(torch.randn(out_features, in_features))\n",
    "        self.bias = torch.nn.Parameter(torch.randn(out_features))\n",
    "        self.rounding_mode = rounding_mode\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Quantize weights and input\n",
    "        q_weight = self.quantizer.quantize(self.weight, self.rounding_mode)\n",
    "        q_input = self.quantizer.quantize(x, self.rounding_mode)\n",
    "        \n",
    "        # Perform computation\n",
    "        output = torch.matmul(q_input, q_weight.t())\n",
    "        \n",
    "        # Quantize bias and add\n",
    "        q_bias = self.quantizer.quantize(self.bias, self.rounding_mode)\n",
    "        output = output + q_bias\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Define some common formats\n",
    "    formats = {\n",
    "        \"fp32\": (8, 23),    # Standard IEEE 754 float32\n",
    "        \"fp16\": (5, 10),    # Standard IEEE 754 float16\n",
    "        \"bf16\": (8, 7),     # Brain Float 16\n",
    "        \"fp8\": (5, 2),      # Example 8-bit float\n",
    "    }\n",
    "    \n",
    "    # Test different rounding modes\n",
    "    rounding_modes = [\n",
    "        \"nearest\",\n",
    "        \"up\",\n",
    "        \"down\",\n",
    "        \"towards_zero\",\n",
    "        \"stochastic_equal\",\n",
    "        \"stochastic_proportional\"\n",
    "    ]\n",
    "    \n",
    "    # Create a sample tensor\n",
    "    x = torch.tensor([0.1, 0.3, 1.7, 3.9, -2.5])\n",
    "    \n",
    "    # Test quantization\n",
    "    simulator = FloatPrecisionSimulator(*formats[\"fp16\"])\n",
    "    \n",
    "    print(\"Original values:\", x)\n",
    "    for mode in rounding_modes:\n",
    "        result = simulator.quantize(x, mode)\n",
    "        print(f\"{mode}:\", result)\n",
    "\n",
    "    # Test with a layer\n",
    "    layer = QuantizedLayer(4, 2, *formats[\"bf16\"], rounding_mode=\"nearest\")\n",
    "    input_tensor = torch.randn(3, 4)\n",
    "    output = layer(input_tensor)\n",
    "    print(\"\\nLayer output shape:\", output.shape)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3be6bfba-0076-4b8f-97fa-11bf07532736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input values:       tensor([ 1.7641,  0.3097, -0.2021,  2.4700,  0.3300])\n",
      "PyTorch FP16:       tensor([ 1.7637,  0.3098, -0.2021,  2.4707,  0.3301])\n",
      "\n",
      "Your output:\n",
      "nearest             : tensor([ 1.7637,  0.3098, -0.2021,  2.4707,  0.3301])\n",
      "up                  : tensor([ 1.7646,  0.3098, -0.2020,  2.4707,  0.3301])\n",
      "down                : tensor([ 1.7637,  0.3096, -0.2021,  2.4688,  0.3298])\n",
      "towards_zero        : tensor([ 1.7637,  0.3096, -0.2020,  2.4688,  0.3298])\n",
      "\n",
      "Correct ones:\n",
      "nearest             : tensor([ 1.7637,  0.3098, -0.2021,  2.4707])\n",
      "up                  : tensor([ 1.7646,  0.3098, -0.2020,  2.4707])\n",
      "down                : tensor([ 1.7637,  0.3096, -0.2021,  2.4688])\n",
      "towards_zero        : tensor([ 1.7637,  0.3096, -0.2020,  2.4688])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "class FloatPrecisionSimulator:\n",
    "    \"\"\"\n",
    "    A class to simulate different floating-point precisions and rounding modes\n",
    "    for PyTorch tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, exponent_bits: int, mantissa_bits: int):\n",
    "        self.exponent_bits = exponent_bits\n",
    "        self.mantissa_bits = mantissa_bits\n",
    "        self.max_exp = 2 ** (exponent_bits - 1) - 1\n",
    "        self.min_exp = -self.max_exp + 1\n",
    "        self.bias = 2 ** (exponent_bits - 1) - 1  # Bias for IEEE 754\n",
    "        \n",
    "    def _to_custom_float(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, \n",
    "                                                        torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Initialize with specific format parameters.\n",
    "        Convert to custom float representation with proper IEEE 754 handling\n",
    "        \n",
    "        Args:\n",
    "            exponent_bits: Number of bits for exponent\n",
    "            mantissa_bits: Number of bits for mantissa (significant digits)\n",
    "        \"\"\"\n",
    "        sign = torch.sign(x)\n",
    "        abs_x = torch.abs(x)\n",
    "        \n",
    "        # Handle special cases\n",
    "        zero_mask = (abs_x == 0)\n",
    "        inf_mask = torch.isinf(x)\n",
    "        nan_mask = torch.isnan(x)\n",
    "        \n",
    "        # Calculate raw exponent and mantissa\n",
    "        exponent = torch.floor(torch.log2(abs_x.clamp(min=2.0**-24)))  # Minimum denormal\n",
    "        \n",
    "        # Normalize mantissa to [1, 2)\n",
    "        mantissa = abs_x / (2.0 ** exponent)\n",
    "        \n",
    "        # Handle subnormals\n",
    "        subnormal_mask = (exponent < self.min_exp)\n",
    "        if torch.any(subnormal_mask):\n",
    "            mantissa[subnormal_mask] = abs_x[subnormal_mask] / (2.0 ** self.min_exp)\n",
    "            exponent[subnormal_mask] = self.min_exp\n",
    "        \n",
    "        return sign, exponent + self.bias, mantissa, zero_mask, inf_mask, nan_mask\n",
    "    \n",
    "    def _quantize_components(self, \n",
    "                           x: torch.Tensor,\n",
    "                           sign: torch.Tensor, \n",
    "                           exponent: torch.Tensor, \n",
    "                           mantissa: torch.Tensor,\n",
    "                           zero_mask: torch.Tensor,\n",
    "                           inf_mask: torch.Tensor,\n",
    "                           nan_mask: torch.Tensor,\n",
    "                           rounding_mode: str) -> torch.Tensor:\n",
    "        \"\"\"Quantize components according to IEEE 754 FP16 rules with various rounding modes\"\"\"\n",
    "        \n",
    "        # Clamp exponent to representable range (including bias)\n",
    "        exp_min = 0  # 0 represents subnormals\n",
    "        exp_max = 2**self.exponent_bits - 1  # 31 for FP16\n",
    "        exponent = exponent.clamp(min=exp_min, max=exp_max)\n",
    "        \n",
    "        # Quantize mantissa\n",
    "        mantissa_steps = 2 ** self.mantissa_bits\n",
    "        normal_mask = (exponent > 0) & (exponent < exp_max)\n",
    "        subnormal_mask = (exponent == 0)\n",
    "        mantissa_normal = mantissa - 1.0  # Remove implicit leading 1 for normal numbers\n",
    "        \n",
    "        # Apply rounding mode\n",
    "        if rounding_mode == \"nearest\":\n",
    "            mantissa_q = torch.round(mantissa_normal * mantissa_steps) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.round(mantissa[subnormal_mask] * \n",
    "                                                       mantissa_steps) / mantissa_steps\n",
    "        elif rounding_mode == \"up\":\n",
    "            mantissa_q = torch.where(sign > 0, \n",
    "                                   torch.ceil(mantissa_normal * mantissa_steps),\n",
    "                                   torch.floor(mantissa_normal * mantissa_steps)) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.where(sign[subnormal_mask] > 0,\n",
    "                                                       torch.ceil(mantissa[subnormal_mask] * mantissa_steps),\n",
    "                                                       torch.floor(mantissa[subnormal_mask] * mantissa_steps)) / mantissa_steps\n",
    "        elif rounding_mode == \"down\":\n",
    "            mantissa_q = torch.where(sign > 0,\n",
    "                                   torch.floor(mantissa_normal * mantissa_steps),\n",
    "                                   torch.ceil(mantissa_normal * mantissa_steps)) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.where(sign[subnormal_mask] > 0,\n",
    "                                                       torch.floor(mantissa[subnormal_mask] * mantissa_steps),\n",
    "                                                       torch.ceil(mantissa[subnormal_mask] * mantissa_steps)) / mantissa_steps\n",
    "        elif rounding_mode == \"towards_zero\":\n",
    "            mantissa_q = torch.trunc(mantissa_normal * mantissa_steps) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.trunc(mantissa[subnormal_mask] * \n",
    "                                                       mantissa_steps) / mantissa_steps\n",
    "        elif rounding_mode == \"stochastic_equal\":\n",
    "            mantissa_scaled = mantissa_normal * mantissa_steps\n",
    "            floor_val = torch.floor(mantissa_scaled)\n",
    "            prob = torch.rand_like(mantissa_scaled)\n",
    "            mantissa_q = torch.where(prob < 0.5, floor_val, floor_val + 1) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_scaled = mantissa[subnormal_mask] * mantissa_steps\n",
    "                floor_val = torch.floor(mantissa_scaled)\n",
    "                prob = torch.rand_like(mantissa_scaled)\n",
    "                mantissa_q[subnormal_mask] = torch.where(prob < 0.5, floor_val, \n",
    "                                                       floor_val + 1) / mantissa_steps\n",
    "        elif rounding_mode == \"stochastic_proportional\":\n",
    "            mantissa_scaled = mantissa_normal * mantissa_steps\n",
    "            floor_val = torch.floor(mantissa_scaled)\n",
    "            fraction = mantissa_scaled - floor_val\n",
    "            prob = torch.rand_like(mantissa_scaled)\n",
    "            mantissa_q = torch.where(prob < fraction, floor_val + 1, floor_val) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_scaled = mantissa[subnormal_mask] * mantissa_steps\n",
    "                floor_val = torch.floor(mantissa_scaled)\n",
    "                fraction = mantissa_scaled - floor_val\n",
    "                prob = torch.rand_like(mantissa_scaled)\n",
    "                mantissa_q[subnormal_mask] = torch.where(prob < fraction, floor_val + 1, \n",
    "                                                       floor_val) / mantissa_steps\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported rounding mode: {rounding_mode}\")\n",
    "        \n",
    "        # Reconstruct the number\n",
    "        result = torch.zeros_like(x)\n",
    "        \n",
    "        # Normal numbers\n",
    "        if torch.any(normal_mask):\n",
    "            result[normal_mask] = sign[normal_mask] * (1.0 + mantissa_q[normal_mask]) * \\\n",
    "                                (2.0 ** (exponent[normal_mask] - self.bias))\n",
    "        \n",
    "        # Subnormal numbers\n",
    "        if torch.any(subnormal_mask):\n",
    "            result[subnormal_mask] = sign[subnormal_mask] * mantissa_q[subnormal_mask] * \\\n",
    "                                   (2.0 ** self.min_exp)\n",
    "        \n",
    "        # Special cases\n",
    "        result[zero_mask] = 0.0\n",
    "        result[inf_mask] = torch.sign(x[inf_mask]) * float('inf')\n",
    "        result[nan_mask] = float('nan')\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def quantize(self, x: torch.Tensor, rounding_mode: str = \"nearest\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Quantize a tensor to the specified precision with given rounding mode.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            rounding_mode: One of 'nearest', 'up', 'down', 'towards_zero', \n",
    "                          'stochastic_equal', 'stochastic_proportional'\n",
    "        \"\"\"\n",
    "        sign, exponent, mantissa, zero_mask, inf_mask, nan_mask = self._to_custom_float(x)\n",
    "        return self._quantize_components(x, sign, exponent, mantissa, zero_mask, inf_mask, nan_mask, rounding_mode)\n",
    "\n",
    "# Test the implementation\n",
    "def test_fp16():\n",
    "    # Test values\n",
    "    values = torch.tensor([1.7641, 0.3097, -0.2021, 2.4700, 0.3300])\n",
    "    \n",
    "    # FP16 simulator (5 exponent bits, 10 mantissa bits)\n",
    "    fp16_sim = FloatPrecisionSimulator(5, 10)\n",
    "    \n",
    "    # Test all rounding modes\n",
    "    rounding_modes = [\"nearest\", \"up\", \"down\", \"towards_zero\", \n",
    "                     \"stochastic_equal\", \"stochastic_proportional\"]\n",
    "    \n",
    "    # Compare with PyTorch's native FP16\n",
    "    fp16_native = values.to(dtype=torch.float16).to(dtype=torch.float32)\n",
    "    \n",
    "    print(\"Input values:      \", values)\n",
    "    print(\"PyTorch FP16:      \", fp16_native)\n",
    "    print()\n",
    "\n",
    "    print(\"Your output:\")\n",
    "    for mode in rounding_modes[:4]:\n",
    "        result = fp16_sim.quantize(values, mode)\n",
    "        print(f\"{mode:20}: {result}\")\n",
    "    \n",
    "\n",
    "    print()\n",
    "    rounding_modes_num = [1, 2, 3, 4, \n",
    "                     \"stochastic_equal\", \"stochastic_proportional\"]\n",
    "\n",
    "    print(\"Correct ones:\")\n",
    "    for mode in rounding_modes_num[:4]:\n",
    "        pyq_f = chop('h', rmode=mode)\n",
    "   \n",
    "        X_bit = pyq_f(X_th[:4, 0])\n",
    "        print(f\"{rounding_modes[mode-1]:20}: {X_bit}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "145f815f-c59c-4c88-8f08-ea2ea49c01fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input values:       tensor([ 1.7641,  0.3097, -0.2021,  2.4700,  0.3300])\n",
      "PyTorch FP16:       tensor([ 1.7637,  0.3098, -0.2021,  2.4707,  0.3301])\n",
      "\n",
      "\n",
      "Correct ones:\n",
      "nearest     ,  Truth:    tensor([ 1.7637,  0.3098, -0.2021,  2.4707,  0.3301])\n",
      "nearest     ,  Emulated: tensor([ 1.7637,  0.3098, -0.2021,  2.4707,  0.3301])\n",
      "up          ,  Truth:    tensor([ 1.7646,  0.3098, -0.2020,  2.4707,  0.3301])\n",
      "up          ,  Emulated: tensor([ 1.7646,  0.3098, -0.2020,  2.4707,  0.3301])\n",
      "down        ,  Truth:    tensor([ 1.7637,  0.3096, -0.2021,  2.4688,  0.3298])\n",
      "down        ,  Emulated: tensor([ 1.7637,  0.3096, -0.2021,  2.4688,  0.3298])\n",
      "towards_zero,  Truth:    tensor([ 1.7637,  0.3096, -0.2020,  2.4688,  0.3298])\n",
      "towards_zero,  Emulated: tensor([ 1.7637,  0.3096, -0.2020,  2.4688,  0.3298])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "class FloatPrecisionSimulator:\n",
    "    \"\"\"\n",
    "    A class to simulate different floating-point precisions and rounding modes\n",
    "    for PyTorch tensors.\n",
    "    \"\"\"\n",
    "    def __init__(self, exponent_bits: int, mantissa_bits: int):\n",
    "        self.exponent_bits = exponent_bits\n",
    "        self.mantissa_bits = mantissa_bits\n",
    "        self.max_exp = 2 ** (exponent_bits - 1) - 1\n",
    "        self.min_exp = -self.max_exp + 1\n",
    "        self.bias = 2 ** (exponent_bits - 1) - 1  # Bias for IEEE 754\n",
    "        \n",
    "    def _to_custom_float(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, \n",
    "                                                        torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Initialize with specific format parameters.\n",
    "        Convert to custom float representation with proper IEEE 754 handling\n",
    "        \n",
    "        Args:\n",
    "            exponent_bits: Number of bits for exponent\n",
    "            mantissa_bits: Number of bits for mantissa (significant digits)\n",
    "        \"\"\"\n",
    "        sign = torch.sign(x)\n",
    "        abs_x = torch.abs(x)\n",
    "        \n",
    "        # Handle special cases\n",
    "        zero_mask = (abs_x == 0)\n",
    "        inf_mask = torch.isinf(x)\n",
    "        nan_mask = torch.isnan(x)\n",
    "        \n",
    "        # Calculate raw exponent and mantissa\n",
    "        exponent = torch.floor(torch.log2(abs_x.clamp(min=2.0**-24)))  # Minimum denormal\n",
    "        \n",
    "        # Normalize mantissa to [1, 2)\n",
    "        mantissa = abs_x / (2.0 ** exponent)\n",
    "        \n",
    "        # Handle subnormals\n",
    "        subnormal_mask = (exponent < self.min_exp)\n",
    "        if torch.any(subnormal_mask):\n",
    "            mantissa[subnormal_mask] = abs_x[subnormal_mask] / (2.0 ** self.min_exp)\n",
    "            exponent[subnormal_mask] = self.min_exp\n",
    "        \n",
    "        return sign, exponent + self.bias, mantissa, zero_mask, inf_mask, nan_mask\n",
    "    \n",
    "    def _quantize_components(self, \n",
    "                           x: torch.Tensor,\n",
    "                           sign: torch.Tensor, \n",
    "                           exponent: torch.Tensor, \n",
    "                           mantissa: torch.Tensor,\n",
    "                           zero_mask: torch.Tensor,\n",
    "                           inf_mask: torch.Tensor,\n",
    "                           nan_mask: torch.Tensor,\n",
    "                           rounding_mode: str) -> torch.Tensor:\n",
    "        \"\"\"Quantize components according to IEEE 754 FP16 rules with various rounding modes\"\"\"\n",
    "        \n",
    "        # Clamp exponent to representable range (including bias)\n",
    "        exp_min = 0  # 0 represents subnormals\n",
    "        exp_max = 2**self.exponent_bits - 1  # 31 for FP16\n",
    "        exponent = exponent.clamp(min=exp_min, max=exp_max)\n",
    "        \n",
    "        # Quantize mantissa\n",
    "        mantissa_steps = 2 ** self.mantissa_bits\n",
    "        normal_mask = (exponent > 0) & (exponent < exp_max)\n",
    "        subnormal_mask = (exponent == 0)\n",
    "        mantissa_normal = mantissa - 1.0  # Remove implicit leading 1 for normal numbers\n",
    "        \n",
    "        # Apply rounding mode\n",
    "        if rounding_mode == \"nearest\":\n",
    "            mantissa_q = torch.round(mantissa_normal * mantissa_steps) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.round(mantissa[subnormal_mask] * \n",
    "                                                       mantissa_steps) / mantissa_steps\n",
    "        elif rounding_mode == \"up\":\n",
    "            mantissa_q = torch.where(sign > 0, \n",
    "                                   torch.ceil(mantissa_normal * mantissa_steps),\n",
    "                                   torch.floor(mantissa_normal * mantissa_steps)) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.where(sign[subnormal_mask] > 0,\n",
    "                                                       torch.ceil(mantissa[subnormal_mask] * mantissa_steps),\n",
    "                                                       torch.floor(mantissa[subnormal_mask] * mantissa_steps)) / mantissa_steps\n",
    "        elif rounding_mode == \"down\":\n",
    "            mantissa_q = torch.where(sign > 0,\n",
    "                                   torch.floor(mantissa_normal * mantissa_steps),\n",
    "                                   torch.ceil(mantissa_normal * mantissa_steps)) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.where(sign[subnormal_mask] > 0,\n",
    "                                                       torch.floor(mantissa[subnormal_mask] * mantissa_steps),\n",
    "                                                       torch.ceil(mantissa[subnormal_mask] * mantissa_steps)) / mantissa_steps\n",
    "        elif rounding_mode == \"towards_zero\":\n",
    "            mantissa_q = torch.trunc(mantissa_normal * mantissa_steps) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_q[subnormal_mask] = torch.trunc(mantissa[subnormal_mask] * \n",
    "                                                       mantissa_steps) / mantissa_steps\n",
    "        elif rounding_mode == \"stochastic_equal\":\n",
    "            mantissa_scaled = mantissa_normal * mantissa_steps\n",
    "            floor_val = torch.floor(mantissa_scaled)\n",
    "            prob = torch.rand_like(mantissa_scaled)\n",
    "            mantissa_q = torch.where(prob < 0.5, floor_val, floor_val + 1) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_scaled = mantissa[subnormal_mask] * mantissa_steps\n",
    "                floor_val = torch.floor(mantissa_scaled)\n",
    "                prob = torch.rand_like(mantissa_scaled)\n",
    "                mantissa_q[subnormal_mask] = torch.where(prob < 0.5, floor_val, \n",
    "                                                       floor_val + 1) / mantissa_steps\n",
    "        elif rounding_mode == \"stochastic_proportional\":\n",
    "            mantissa_scaled = mantissa_normal * mantissa_steps\n",
    "            floor_val = torch.floor(mantissa_scaled)\n",
    "            fraction = mantissa_scaled - floor_val\n",
    "            prob = torch.rand_like(mantissa_scaled)\n",
    "            mantissa_q = torch.where(prob < fraction, floor_val + 1, floor_val) / mantissa_steps\n",
    "            if torch.any(subnormal_mask):\n",
    "                mantissa_scaled = mantissa[subnormal_mask] * mantissa_steps\n",
    "                floor_val = torch.floor(mantissa_scaled)\n",
    "                fraction = mantissa_scaled - floor_val\n",
    "                prob = torch.rand_like(mantissa_scaled)\n",
    "                mantissa_q[subnormal_mask] = torch.where(prob < fraction, floor_val + 1, \n",
    "                                                       floor_val) / mantissa_steps\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported rounding mode: {rounding_mode}\")\n",
    "        \n",
    "        # Reconstruct the number\n",
    "        result = torch.zeros_like(x)\n",
    "        \n",
    "        # Normal numbers\n",
    "        if torch.any(normal_mask):\n",
    "            result[normal_mask] = sign[normal_mask] * (1.0 + mantissa_q[normal_mask]) * \\\n",
    "                                (2.0 ** (exponent[normal_mask] - self.bias))\n",
    "        \n",
    "        # Subnormal numbers\n",
    "        if torch.any(subnormal_mask):\n",
    "            result[subnormal_mask] = sign[subnormal_mask] * mantissa_q[subnormal_mask] * \\\n",
    "                                   (2.0 ** self.min_exp)\n",
    "        \n",
    "        # Special cases\n",
    "        result[zero_mask] = 0.0\n",
    "        result[inf_mask] = torch.sign(x[inf_mask]) * float('inf')\n",
    "        result[nan_mask] = float('nan')\n",
    "        \n",
    "        return result\n",
    "\n",
    "    def quantize(self, x: torch.Tensor, rounding_mode: str = \"nearest\") -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Quantize a tensor to the specified precision with given rounding mode.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            rounding_mode: One of 'nearest', 'up', 'down', 'towards_zero', \n",
    "                          'stochastic_equal', 'stochastic_proportional'\n",
    "        \"\"\"\n",
    "        sign, exponent, mantissa, zero_mask, inf_mask, nan_mask = self._to_custom_float(x)\n",
    "        return self._quantize_components(x, sign, exponent, mantissa, zero_mask, inf_mask, nan_mask, rounding_mode)\n",
    "\n",
    "# Test the implementation\n",
    "def test_fp16():\n",
    "    # Test values\n",
    "    values = torch.tensor([1.7641, 0.3097, -0.2021, 2.4700, 0.3300])\n",
    "    \n",
    "    # FP16 simulator (5 exponent bits, 10 mantissa bits)\n",
    "    fp16_sim = FloatPrecisionSimulator(5, 10)\n",
    "    \n",
    "    # Test all rounding modes\n",
    "    rounding_modes = [\"nearest\", \"up\", \"down\", \"towards_zero\", \n",
    "                     \"stochastic_equal\", \"stochastic_proportional\"]\n",
    "    \n",
    "    # Compare with PyTorch's native FP16\n",
    "    fp16_native = values.to(dtype=torch.float16).to(dtype=torch.float32)\n",
    "    \n",
    "    print(\"Input values:      \", values)\n",
    "    print(\"PyTorch FP16:      \", fp16_native)\n",
    "    print()\n",
    "\n",
    "    print()\n",
    "    rounding_modes_num = [1, 2, 3, 4, \"stochastic_equal\", \"stochastic_proportional\"]\n",
    "\n",
    "    print(\"Correct ones:\")\n",
    "    for mode in rounding_modes_num[:4]:\n",
    "        pyq_f = chop('h', rmode=mode)\n",
    "        groud_truth = pyq_f(values)\n",
    "        emulated = fp16_sim.quantize(values, rounding_modes[mode-1])\n",
    "        assert np.array_equal(emulated, groud_truth), print(\"error rmode 3\")\n",
    "        \n",
    "        print(f\"{rounding_modes[mode-1]:12}, \", \"Truth:\", f\"   {emulated}\")\n",
    "        print(f\"{rounding_modes[mode-1]:12}, \", \"Emulated:\", f\"{groud_truth}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    test_fp16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b33c5695-d12d-4c8f-9ae8-32762b70f48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Troch backend.\n",
      "runtime: 1.5513670444488525\n",
      "tensor([ 1.7637,  0.3098, -0.2021,  2.4707,  0.3301])\n"
     ]
    }
   ],
   "source": [
    "pychop.backend('torch', 1) # print information\n",
    "pyq_f = chop('h', rmode=1)\n",
    "st = time()\n",
    "X_bit = pyq_f(X_th)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a90d1771-a080-4386-bfd5-0c86bd09db3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Troch backend.\n",
      "runtime: 1.1380090713500977\n",
      "tensor([ 1.7646,  0.3098, -0.2020,  2.4707,  0.3301])\n"
     ]
    }
   ],
   "source": [
    "pychop.backend('torch', 1) # print information\n",
    "pyq_f = chop('h', rmode=2)\n",
    "st = time()\n",
    "X_bit = pyq_f(X_th)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c37848fd-10fc-4789-b1cb-8e1e77ec315d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Troch backend.\n",
      "runtime: 1.0816121101379395\n",
      "tensor([ 1.7637,  0.3096, -0.2021,  2.4688,  0.3298])\n"
     ]
    }
   ],
   "source": [
    "pychop.backend('torch', 1) # print information\n",
    "pyq_f = chop('h', rmode=3)\n",
    "st = time()\n",
    "X_bit = pyq_f(X_th)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4db74eb8-dd68-4673-8b63-e55e426081ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Troch backend.\n",
      "runtime: 1.2739830017089844\n",
      "tensor([ 1.7637,  0.3096, -0.2020,  2.4688,  0.3298])\n"
     ]
    }
   ],
   "source": [
    "pychop.backend('torch', 1) # print information\n",
    "pyq_f = chop('h', rmode=4)\n",
    "st = time()\n",
    "X_bit = pyq_f(X_th)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:5, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "43a4f703-63ff-49a9-adaa-95a0890d95ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.7644,  0.3096, -0.2021,  2.4692,  0.3302,  0.3021,  0.3712,  0.3873,\n",
      "        -1.9394,  0.5616])\n"
     ]
    }
   ],
   "source": [
    "formats = {\n",
    "    \"fp32\": (8, 23),    # Standard IEEE 754 float32\n",
    "    \"fp16\": (5, 10),    # Standard IEEE 754 float16\n",
    "    \"bf16\": (8, 7),     # Brain Float 16\n",
    "    \"fp8\": (5, 2),      # Example 8-bit float\n",
    "}\n",
    "simulator = FloatPrecisionSimulator(*formats[\"fp16\"])\n",
    "X_bit = simulator.quantize(X_th, rounding_mode=\"nearest\")\n",
    "print(X_bit[:10, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b59fd0-ed01-4ac7-98c2-0741e84f032c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load JAX backend.\n",
      "runtime: 7.780159950256348\n",
      "[[ 1.7637  0.4001  0.9785 ...  0.9292  0.2294  0.4143]\n",
      " [ 0.3098 -0.7373 -1.5371 ...  0.5171 -0.0329  1.2979]\n",
      " [-0.2021 -0.833   1.7334 ...  0.7529 -0.5811 -0.1984]\n",
      " ...\n",
      " [ 1.0742  1.1885  0.5093 ...  0.0706  0.5996 -2.4102]\n",
      " [ 0.3242 -0.0234  1.6289 ... -0.1609 -1.5977  1.415 ]\n",
      " [ 0.6348  1.3809  0.5483 ...  0.3076 -0.1108  0.8384]]\n"
     ]
    }
   ],
   "source": [
    "pychop.backend('jax', 1) # print information\n",
    "pyq_f = chop('h')\n",
    "st = time()\n",
    "X_bit = pyq_f(X_jx)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "57b1a7c5-d1c8-4850-8dc4-a1af7b1417a3",
   "metadata": {},
   "source": [
    "pychop.backend('torch')\n",
    "pyq_f = chop('h', device='cuda')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_gpu = X_th.to(device)\n",
    "pyq_f(X_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f79af559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22ecc758",
   "metadata": {},
   "source": [
    "### integer quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c29813b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.800703887228494"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pychop.backend('numpy')\n",
    "pyq_f = pychop.quant(bits=8)\n",
    "X_q = pyq_f(X_np)\n",
    "X_inv = pyq_f.dequant(X_q)\n",
    "linalg.norm(X_inv - X_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef762b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.800703841249906"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pychop.backend('torch')\n",
    "pyq_f = pychop.quant(bits=8)\n",
    "X_q = pyq_f(X_th)\n",
    "X_inv = pyq_f.dequant(X_q)\n",
    "linalg.norm(X_inv - X_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea235d0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.7823"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pychop.backend('jax')\n",
    "pyq_f = pychop.quant(bits=8)\n",
    "X_q = pyq_f(X_jx)\n",
    "X_inv = pyq_f.dequant(X_q)\n",
    "linalg.norm(X_inv - X_jx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151a103",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "00497bc7",
   "metadata": {},
   "source": [
    "### fixed point quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2099b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.75  ,  0.375 ,  1.    , ...,  0.9375,  0.25  ,  0.4375],\n",
       "       [ 0.3125, -0.75  , -1.5625, ...,  0.5   , -0.0625,  1.3125],\n",
       "       [-0.1875, -0.8125,  1.75  , ...,  0.75  , -0.5625, -0.1875],\n",
       "       ...,\n",
       "       [ 1.0625,  1.1875,  0.5   , ...,  0.0625,  0.625 , -2.4375],\n",
       "       [ 0.3125, -0.    ,  1.625 , ..., -0.1875, -1.625 ,  1.4375],\n",
       "       [ 0.625 ,  1.375 ,  0.5625, ...,  0.3125, -0.125 ,  0.8125]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pychop.backend('numpy')\n",
    "pyq_f = pychop.fpoint()\n",
    "\n",
    "pyq_f(X_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a320530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.7500,  0.3750,  1.0000,  ...,  0.9375,  0.2500,  0.4375],\n",
       "        [ 0.3125, -0.7500, -1.5625,  ...,  0.5000, -0.0625,  1.3125],\n",
       "        [-0.1875, -0.8125,  1.7500,  ...,  0.7500, -0.5625, -0.1875],\n",
       "        ...,\n",
       "        [ 1.0625,  1.1875,  0.5000,  ...,  0.0625,  0.6250, -2.4375],\n",
       "        [ 0.3125, -0.0000,  1.6250,  ..., -0.1875, -1.6250,  1.4375],\n",
       "        [ 0.6250,  1.3750,  0.5625,  ...,  0.3125, -0.1250,  0.8125]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pychop.backend('torch')\n",
    "pyq_f = pychop.fpoint()\n",
    "pyq_f(X_th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f58d8820",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 1.75  ,  0.375 ,  1.    , ...,  0.9375,  0.25  ,  0.4375],\n",
       "       [ 0.3125, -0.75  , -1.5625, ...,  0.5   , -0.0625,  1.3125],\n",
       "       [-0.1875, -0.8125,  1.75  , ...,  0.75  , -0.5625, -0.1875],\n",
       "       ...,\n",
       "       [ 1.0625,  1.1875,  0.5   , ...,  0.0625,  0.625 , -2.4375],\n",
       "       [ 0.3125, -0.    ,  1.625 , ..., -0.1875, -1.625 ,  1.4375],\n",
       "       [ 0.625 ,  1.375 ,  0.5625, ...,  0.3125, -0.125 ,  0.8125]],      dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pychop.backend('jax')\n",
    "pyq_f = pychop.fpoint()\n",
    "pyq_f(X_jx)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7b02be34-c7b5-462c-92b4-f977e746ad53",
   "metadata": {},
   "source": [
    "from pychop.bitchop import bitchop\n",
    "pychop.backend('numpy')\n",
    "\n",
    "pyq_f = bitchop(exp_bits=5, sig_bits=10, rmode=1, subnormal=True, random_state=42, device=\"cpu\", verbose=0)\n",
    "st = time()\n",
    "X_bit = pyq_f(X_np)\n",
    "print(\"runtime:\", time() - st)\n",
    "print(X_bit[:10, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31030621-0cd6-4a07-8376-1e1d9e3ebd06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf31fc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05eca45-ff22-4368-a68a-8fd4c6728f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
