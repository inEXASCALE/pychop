{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad8361ea-4665-44ed-8526-617e1ded6b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# appending a path\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pychop\n",
    "from pychop import Chopi\n",
    "pychop.backend('torch')\n",
    "\n",
    "class QuantizedNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizedNet, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv1d = nn.Conv1d(1, 16, 3, padding=1)\n",
    "        self.conv2d = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv3d = nn.Conv3d(1, 16, 3, padding=1)\n",
    "\n",
    "        self.wquant_conv1d = Chopi(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_conv2d = Chopi(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_conv3d = Chopi(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "\n",
    "        # Recurrent Layers\n",
    "        self.lstm = nn.LSTM(16, 32, batch_first=True)\n",
    "        self.gru = nn.GRU(32, 16, batch_first=True)\n",
    "\n",
    "        self.wquant_lstm_ih = Chopi(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_lstm_hh = Chopi(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_gru_ih = Chopi(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_gru_hh = Chopi(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "\n",
    "        # BatchNorm, Pooling, and Linear\n",
    "        self.bn2d = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(16 * 14 * 14, 10)  # 14x14 after pooling\n",
    "        self.wquant_fc = Chopi(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "\n",
    "        # Activation Chopis\n",
    "        self.aquant_conv1d = Chopi(8, symmetric=False, per_channel=True, channel_dim=1)\n",
    "        self.aquant_conv2d = Chopi(8, symmetric=False, per_channel=True, channel_dim=1)\n",
    "        self.aquant_conv3d = Chopi(8, symmetric=False, per_channel=True, channel_dim=1)\n",
    "        self.aquant_lstm = Chopi(8, symmetric=False, per_channel=True, channel_dim=2)  # dim 2 for [batch, seq, feat]\n",
    "        self.aquant_gru = Chopi(8, symmetric=False, per_channel=True, channel_dim=2)\n",
    "        self.aquant_fc = Chopi(8, symmetric=False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_1d, x_2d, x_3d, x_seq, training=True):\n",
    "        # Conv1d\n",
    "        w1d = self.wquant_conv1d(self.conv1d.weight, training=training)\n",
    "\n",
    "        x_1d = F.conv1d(x_1d, w1d, self.conv1d.bias, padding=1)\n",
    "        x_1d = self.aquant_conv1d(x_1d, training=training)\n",
    "        x_1d = self.relu(x_1d)\n",
    "\n",
    "        # Conv2d\n",
    "        w2d = self.wquant_conv2d(self.conv2d.weight, training=training)\n",
    "        x_2d = F.conv2d(x_2d, w2d, self.conv2d.bias, padding=1)\n",
    "        x_2d = self.bn2d(x_2d)\n",
    "        x_2d = self.aquant_conv2d(x_2d, training=training)\n",
    "        x_2d = self.relu(x_2d)\n",
    "        x_2d = self.pool(x_2d)  # [2, 16, 14, 14]\n",
    "        x_2d = x_2d.view(x_2d.size(0), -1)  # [2, 16*14*14]\n",
    "\n",
    "        # Conv3d\n",
    "        w3d = self.wquant_conv3d(self.conv3d.weight, training=training)\n",
    "        x_3d = F.conv3d(x_3d, w3d, self.conv3d.bias, padding=1)\n",
    "        x_3d = self.aquant_conv3d(x_3d, training=training)\n",
    "        x_3d = self.relu(x_3d)\n",
    "\n",
    "        # RNN (LSTM + GRU)\n",
    "        # Note: PyTorch RNNs use fused ops, so we quantize weights but apply them manually is complex.\n",
    "        # For simplicity, we quantize inputs/outputs here; true integer RNNs need custom ops.\n",
    "        w_lstm_ih = self.wquant_lstm_ih(self.lstm.weight_ih_l0, training=training)\n",
    "        w_lstm_hh = self.wquant_lstm_hh(self.lstm.weight_hh_l0, training=training)\n",
    "        x_seq, _ = self.lstm(x_seq)  # Fused op, weights not directly applied here\n",
    "        x_seq = self.aquant_lstm(x_seq, training=training)\n",
    "        w_gru_ih = self.wquant_gru_ih(self.gru.weight_ih_l0, training=training)\n",
    "        w_gru_hh = self.wquant_gru_hh(self.gru.weight_hh_l0, training=training)\n",
    "        x_seq, _ = self.gru(x_seq)\n",
    "        x_seq = self.aquant_gru(x_seq, training=training)\n",
    "        x_seq = x_seq[:, -1, :]  # Last timestep\n",
    "\n",
    "        # Linear\n",
    "        w_fc = self.wquant_fc(self.fc.weight, training=training)\n",
    "        x_2d = F.linear(x_2d, w_fc, self.fc.bias)  # [2, 3136] * [3136, 10]\n",
    "        x_2d = self.aquant_fc(x_2d, training=training)\n",
    "\n",
    "        return x_1d, x_2d, x_3d, x_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b1c08d6-9908-44b8-91df-1a268e3948fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training outputs:\n",
      "Conv1d: torch.Size([2, 16, 64]) tensor([0.0000, 0.1261, 0.1855, 0.0000, 0.5060])\n",
      "Conv2d+FC: torch.Size([2, 10]) tensor([ 0.5681, -0.1191, -0.4265, -0.8062,  0.1160])\n",
      "Conv3d: torch.Size([2, 16, 16, 16, 16]) tensor([0.0985, 0.0000, 0.0000, 0.4210, 0.0000])\n",
      "LSTM+GRU: torch.Size([2, 16]) tensor([-0.2229, -0.1467, -0.0997,  0.1245,  0.3705])\n",
      "\n",
      "Inference outputs (INT8):\n",
      "Conv1d: torch.Size([2, 16, 64]) tensor([46., 57., 62., 36., 89.])\n",
      "Conv2d+FC: torch.Size([2, 10]) tensor([70., 72., 24.,  7., 93.])\n",
      "Conv3d: torch.Size([2, 16, 16, 16, 16]) tensor([[67., 62., 46., 77., 55., 66., 59., 75., 65., 68., 64., 65., 36., 93.,\n",
      "         70., 65.],\n",
      "        [57., 56., 84., 53., 68., 62., 75., 56., 51., 64., 51., 56., 88., 43.,\n",
      "         64., 54.],\n",
      "        [52., 70., 45., 74., 61., 65., 73., 45., 57., 45., 82., 41., 56., 58.,\n",
      "         56., 60.],\n",
      "        [68., 51., 70., 64., 84., 48., 31., 74., 56., 83., 50., 79., 50., 59.,\n",
      "         71., 66.],\n",
      "        [55., 77., 61., 63., 61., 45., 56., 58., 88., 31., 91., 36., 74., 74.,\n",
      "         45., 57.]])\n",
      "LSTM+GRU: torch.Size([2, 16]) tensor([  0.,   0., 127.,   0.,   0.])\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    model = QuantizedNet()\n",
    "    model.train()\n",
    "\n",
    "    x_1d = torch.randn(2, 1, 64)  # [batch, channels, length]\n",
    "    x_2d = torch.randn(2, 1, 28, 28)  # [batch, channels, height, width]\n",
    "    x_3d = torch.randn(2, 1, 16, 16, 16)  # [batch, channels, depth, height, width]\n",
    "    x_seq = torch.randn(2, 10, 16)  # [batch, seq_len, features]\n",
    "\n",
    "    # Training mode\n",
    "    out_1d, out_2d, out_3d, out_seq = model(x_1d, x_2d, x_3d, x_seq, training=True)\n",
    "    print(\"Training outputs:\")\n",
    "    print(\"Conv1d:\", out_1d.shape, out_1d[0, 0, :5])\n",
    "    print(\"Conv2d+FC:\", out_2d.shape, out_2d[0, :5])\n",
    "    print(\"Conv3d:\", out_3d.shape, out_3d[0, 0, 0, 0, :5])\n",
    "    print(\"LSTM+GRU:\", out_seq.shape, out_seq[0, :5])\n",
    "\n",
    "    # Inference mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_1d, out_2d, out_3d, out_seq = model(x_1d, x_2d, x_3d, x_seq, training=False)\n",
    "        print(\"\\nInference outputs (INT8):\")\n",
    "        print(\"Conv1d:\", out_1d.shape, out_1d[0, 0, :5])\n",
    "        print(\"Conv2d+FC:\", out_2d.shape, out_2d[0, :5])\n",
    "        print(\"Conv3d:\", out_3d.shape, out_3d[0, 0, 0, :5])\n",
    "        print(\"LSTM+GRU:\", out_seq.shape, out_seq[0, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea680bb3-37f8-4830-9b4c-b8f85c5bfc3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "673003f5-09f3-44e7-b664-9e4b2e7a8e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "# appending a path\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pychop\n",
    "from pychop.layers import IntQuantizedLayer # Without the need to specify pychop.backend('torch')\n",
    "\n",
    "from pychop import Chopi\n",
    "\n",
    "class QuantizedNet2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(QuantizedNet2, self).__init__()\n",
    "        # Convolutional Layers\n",
    "        self.conv1d = nn.Conv1d(1, 16, 3, padding=1)\n",
    "        self.conv2d = nn.Conv2d(1, 16, 3, padding=1)\n",
    "        self.conv3d = nn.Conv3d(1, 16, 3, padding=1)\n",
    "\n",
    "        self.wquant_conv1d = IntQuantizedLayer(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_conv2d = IntQuantizedLayer(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_conv3d = IntQuantizedLayer(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "\n",
    "        # Recurrent Layers\n",
    "        self.lstm = nn.LSTM(16, 32, batch_first=True)\n",
    "        self.gru = nn.GRU(32, 16, batch_first=True)\n",
    "\n",
    "        self.wquant_lstm_ih = IntQuantizedLayer(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_lstm_hh = IntQuantizedLayer(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_gru_ih = IntQuantizedLayer(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "        self.wquant_gru_hh = IntQuantizedLayer(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "\n",
    "        # BatchNorm, Pooling, and Linear\n",
    "        self.bn2d = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(16 * 14 * 14, 10)  # 14x14 after pooling\n",
    "        self.wquant_fc = IntQuantizedLayer(8, symmetric=True, per_channel=True, channel_dim=0)\n",
    "\n",
    "        # Activation IntQuantizedLayers\n",
    "        self.aquant_conv1d = IntQuantizedLayer(8, symmetric=False, per_channel=True, channel_dim=1)\n",
    "        self.aquant_conv2d = IntQuantizedLayer(8, symmetric=False, per_channel=True, channel_dim=1)\n",
    "        self.aquant_conv3d = IntQuantizedLayer(8, symmetric=False, per_channel=True, channel_dim=1)\n",
    "        self.aquant_lstm = IntQuantizedLayer(8, symmetric=False, per_channel=True, channel_dim=2)  # dim 2 for [batch, seq, feat]\n",
    "        self.aquant_gru = IntQuantizedLayer(8, symmetric=False, per_channel=True, channel_dim=2)\n",
    "        self.aquant_fc = IntQuantizedLayer(8, symmetric=False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x_1d, x_2d, x_3d, x_seq):\n",
    "        w1d = self.wquant_conv1d(self.conv1d.weight)\n",
    "\n",
    "        x_1d = F.conv1d(x_1d, w1d, self.conv1d.bias, padding=1)\n",
    "        x_1d = self.aquant_conv1d(x_1d)\n",
    "        x_1d = self.relu(x_1d)\n",
    "\n",
    "        w2d = self.wquant_conv2d(self.conv2d.weight)\n",
    "        x_2d = F.conv2d(x_2d, w2d, self.conv2d.bias, padding=1)\n",
    "        x_2d = self.bn2d(x_2d)\n",
    "        x_2d = self.aquant_conv2d(x_2d)\n",
    "        x_2d = self.relu(x_2d)\n",
    "        x_2d = self.pool(x_2d)  # [2, 16, 14, 14]\n",
    "        x_2d = x_2d.view(x_2d.size(0), -1)  # [2, 16*14*14]\n",
    "\n",
    "        w3d = self.wquant_conv3d(self.conv3d.weight)\n",
    "        x_3d = F.conv3d(x_3d, w3d, self.conv3d.bias, padding=1)\n",
    "        x_3d = self.aquant_conv3d(x_3d)\n",
    "        x_3d = self.relu(x_3d)\n",
    "\n",
    "        w_lstm_ih = self.wquant_lstm_ih(self.lstm.weight_ih_l0)\n",
    "        w_lstm_hh = self.wquant_lstm_hh(self.lstm.weight_hh_l0)\n",
    "        x_seq, _ = self.lstm(x_seq)  # Fused op, weights not directly applied here\n",
    "        x_seq = self.aquant_lstm(x_seq)\n",
    "        w_gru_ih = self.wquant_gru_ih(self.gru.weight_ih_l0)\n",
    "        w_gru_hh = self.wquant_gru_hh(self.gru.weight_hh_l0)\n",
    "        x_seq, _ = self.gru(x_seq)\n",
    "        x_seq = self.aquant_gru(x_seq)\n",
    "        x_seq = x_seq[:, -1, :]  # Last timestep\n",
    "\n",
    "        # Linear\n",
    "        w_fc = self.wquant_fc(self.fc.weight)\n",
    "        x_2d = F.linear(x_2d, w_fc, self.fc.bias)  # [2, 3136] * [3136, 10]\n",
    "        x_2d = self.aquant_fc(x_2d)\n",
    "\n",
    "        return x_1d, x_2d, x_3d, x_seq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd4c28d6-8e2d-4dee-984a-6da72bdcb6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training outputs:\n",
      "Conv1d: torch.Size([2, 16, 64]) tensor([89., 48., 12., 45., 42.])\n",
      "Conv2d+FC: torch.Size([2, 10]) tensor([54., 25.,  0., 39., 42.])\n",
      "Conv3d: torch.Size([2, 16, 16, 16, 16]) tensor([80., 87., 60., 43., 58.])\n",
      "LSTM+GRU: torch.Size([2, 16]) tensor([  0.,   0., 127.,   0.,   0.])\n",
      "\n",
      "Inference outputs (INT8):\n",
      "Conv1d: torch.Size([2, 16, 64]) tensor([89., 48., 12., 45., 42.])\n",
      "Conv2d+FC: torch.Size([2, 10]) tensor([54., 25.,  0., 39., 42.])\n",
      "Conv3d: torch.Size([2, 16, 16, 16, 16]) tensor([[ 80.,  87.,  60.,  43.,  58.,  46.,  44.,  48.,  53.,  69.,  69.,  66.,\n",
      "          62.,  57.,  66.,  80.],\n",
      "        [ 68.,  32., 103.,  68.,  62.,  73.,  94.,  79.,  65.,  46.,  63.,  98.,\n",
      "          61.,  52.,  61.,  78.],\n",
      "        [ 51.,  56., 102.,  79.,  62.,  56.,  37.,  54.,  62.,  76.,  67.,  19.,\n",
      "          71.,  72.,  53.,  88.],\n",
      "        [ 69.,  67.,  78.,  63.,  63.,  40.,  28.,  47.,  64.,  84.,  58.,  59.,\n",
      "          74.,  69.,  60., 100.],\n",
      "        [ 28.,  48.,  46.,  46.,  38.,  82.,  48.,  59.,  56.,  64.,  65.,  80.,\n",
      "          81.,  53.,  68.,  68.]])\n",
      "LSTM+GRU: torch.Size([2, 16]) tensor([  0.,   0., 127.,   0.,   0.])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model = QuantizedNet2()\n",
    "    model.train()\n",
    "\n",
    "    x_1d = torch.randn(2, 1, 64)  # [batch, channels, length]\n",
    "    x_2d = torch.randn(2, 1, 28, 28)  # [batch, channels, height, width]\n",
    "    x_3d = torch.randn(2, 1, 16, 16, 16)  # [batch, channels, depth, height, width]\n",
    "    x_seq = torch.randn(2, 10, 16)  # [batch, seq_len, features]\n",
    "\n",
    "    # Training mode\n",
    "    out_1d, out_2d, out_3d, out_seq = model(x_1d, x_2d, x_3d, x_seq)\n",
    "    print(\"Training outputs:\")\n",
    "    print(\"Conv1d:\", out_1d.shape, out_1d[0, 0, :5])\n",
    "    print(\"Conv2d+FC:\", out_2d.shape, out_2d[0, :5])\n",
    "    print(\"Conv3d:\", out_3d.shape, out_3d[0, 0, 0, 0, :5])\n",
    "    print(\"LSTM+GRU:\", out_seq.shape, out_seq[0, :5])\n",
    "\n",
    "    # Inference mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out_1d, out_2d, out_3d, out_seq = model(x_1d, x_2d, x_3d, x_seq)\n",
    "        print(\"\\nInference outputs (INT8):\")\n",
    "        print(\"Conv1d:\", out_1d.shape, out_1d[0, 0, :5])\n",
    "        print(\"Conv2d+FC:\", out_2d.shape, out_2d[0, :5])\n",
    "        print(\"Conv3d:\", out_3d.shape, out_3d[0, 0, 0, :5])\n",
    "        print(\"LSTM+GRU:\", out_seq.shape, out_seq[0, :5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba92f7f1-0f9d-4ab1-bfe4-c0f16a44f1a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755bd94-7407-42fb-a3ec-faa3bc147b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116017f5-ac11-4fbc-b96b-88485e67219b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
